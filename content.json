{"meta":{"title":"Charm's Blog","subtitle":null,"description":"编程学习实践blog","author":"Charm Sun","url":"https://charmsun.github.io"},"pages":[],"posts":[{"title":"微信小程序实践总结","slug":"weapp-summary","date":"2018-08-31T06:27:06.000Z","updated":"2018-08-31T06:57:22.347Z","comments":true,"path":"2018/08/31/weapp-summary/","link":"","permalink":"https://charmsun.github.io/2018/08/31/weapp-summary/","excerpt":"","text":"依托微信平台，借助于微信的巨大用户量，小程序有着不错的发展趋势，在对微信小程序进行一番探索实践后，做个经验总结。这里就不详细介绍小程序的一些基础知识了，可以仔细研读官方开发文档，再结合demo代码，可以有更好的理解，这里就结合开发中遇到的一些问题进行探讨。 demo代码位于： 微信小程序客户端和服务端Demo 该demo的开发使用了官方提供的带node.js 服务端的开发模板，在使用该模板开发时，还是发现模板中前端和后端sdk包 有不少问题，建议开发时还是参考 官方的sdk 源码做一些自主的修改。 小程序中的图片和图标在小程序开发过程中，在wxss中想使用图片作背景图时，小程序是不支持从本地图片引入的，需要使用远程网络图片，而在wxml 中image 标签则可使用本地图片或网络图片。在小程序打包大小允许的情况下，使用本地图片可减少网络请求，在小程序页面中尽快显示。当需要从网络加载图片时，可以使用开发者自己的静态资源服务器（如果你的网络服务好的话），或者一些图床服务，小程序开发工具中也提供了一个方便的素材管理功能，申请开通后，就可以直接上传至腾讯云的对象存储中，如图所示。 在开发时我们会需要使用到一些图标，小程序官方也提供了一些基础的图标，但显然是不够用的，在web html的开发时我们常将图标合并以字体文件的形式使用，小程序里也是支持的。但同样，字体文件不支持从本地引入，需要从网络加载，这里可以使用阿里的 iconfont 整理好项目的图标，生成代码，从阿里的cdn引入加载，十分方便。可以在wxss中引入字体资源，也可以通过小程序的动态加载字体接口wx.loadFontFace引入。 小程序中的本地存储 storage如官方文档所说，每个微信小程序都可以有自己的本地缓存，同一个微信用户，同一个小程序 storage 上限为 10MB。localStorage 以用户维度隔离，同一台设备上，A 用户无法读取到 B 用户的数据。在浏览器web开发中，我们常用cookie的形式来鉴别http的状态，在小程序中我们则可以使用这个本地存储来 存储一些用户登录信息，购物车数据等，可见demo代码中使用情况。那么使用时也需要十分注意这个本地存储的销毁时间，一般在用户使用小程序时，这个本地存储是一直存在的，有以下几种情形会清空这个存储： 小程序代码主动去清空相关存储 如果用户储存空间不足，微信会清空最近最久未使用的小程序的本地缓存。 用户主动将小程序从微信中删除 小程序的登录问题在开发小程序时，看开发文档后，登录是一个比较令人困惑的问题，关于登录和获取微信用户信息在社区中也是讨论最多的两个问题，官方也做了一些相应的解释。这里我就再总结一下。 在之前小程序启动时，有不少程序员在启动时就调用wx.getUserInfo，这就导致用户在不知道小程序什么样的情况下，就遇到授权弹窗，官方看到不少用户会直接点不允许授权，觉得这种体验方式不好，造成用户流失。因此现在改为不推荐直接调用getUserInfo，而是建议通过按钮点击去让用户授权，获取用户信息，可以让用户先看看小程序长什么样，必要时再去授权。这也造成不少开发者需要去修改代码，有不少吐槽。 官方提供相关开放接口wx.login，wx.getUserInfo和wx.checkSession等，究竟何时需要使用，怎么使用呢。 某些轻量小程序或小游戏不需要登录行为，但是也想获取用户信息，可以不使用wx.login接口，直接通过点按钮授权的方式去获取用户信息，也可以使用wx.getUserInfo设置withCredentials参数获取用户信息。 很多场景下，例如demo中的商店小程序，需要有后端服务，需要接入微信用户，数据库中需要记录订单支付信息，收货地址信息等，这时就需要用到微信提供的wx.login接口。根据官方文档，一个登录的最佳实践是这样的： 可以按该时序图开发登录过程，在这个过程中需要注意的是： 注意定期使用 wx.getUserInfo获取并更新用户的信息，防止微信用户在微信中修改了自己的信息； 目前微信的session_key 有效期是三天，如果开发者设置自己的登录态有效期的话，有效期要小于这个值。不设置的话可以用wx.checkSession去检查是否过期，注意在小程序服务端去更新这个session_key； 这个session_key同时也是解码密钥，不要直接下发到小程序客户端，可以像demo代码中那样在服务端对其进行SHA1加密后再下发到client。 在demo代码中用到了官方提供的基于腾讯云的小程序全栈方案，包括了server端sdk，和client端sdk，发现源码有如下一些问题： 先看client端sdk中封装的request代码，每次当带登录态进行请求时，先调用login接口后再请求，每次login请求都会去走后端的授权中间件部分，对用户信息和session_key进行更新，其实按理是没有必要每次请求都先调login的，也造成请求上的浪费。最好是在必要时去调用login接口更新信息，如登录态失效或session_key过期。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384function request(options) &#123; if (typeof options !== &apos;object&apos;) &#123; var message = &apos;请求传参应为 object 类型，但实际传了 &apos; + (typeof options) + &apos; 类型&apos;; throw new RequestError(constants.ERR_INVALID_PARAMS, message); &#125; var requireLogin = options.login; var success = options.success || noop; var fail = options.fail || noop; var complete = options.complete || noop; var originHeader = options.header || &#123;&#125;; // 成功回调 var callSuccess = function () &#123; success.apply(null, arguments); complete.apply(null, arguments); &#125;; // 失败回调 var callFail = function (error) &#123; fail.call(null, error); complete.call(null, error); &#125;; // 是否已经进行过重试 var hasRetried = false; if (requireLogin) &#123; doRequestWithLogin(); &#125; else &#123; doRequest(); &#125; // 登录后再请求 function doRequestWithLogin() &#123; loginLib.loginWithCode(&#123; success: doRequest, fail: callFail &#125;); &#125; // 实际进行请求的方法 function doRequest() &#123; var authHeader = &#123;&#125; if (requireLogin) &#123; var session = Session.get(); if (!session) &#123; return doRequestWithLogin(); &#125; authHeader = buildAuthHeader(session.skey); &#125; wx.request(utils.extend(&#123;&#125;, options, &#123; header: utils.extend(&#123;&#125;, originHeader, authHeader), success: function (response) &#123; var data = response.data; var error, message; if (data &amp;&amp; data.code === -1) &#123; //Session.clear(); // 如果是登录态无效，并且还没重试过，会尝试登录后刷新凭据重新请求 if (!hasRetried) &#123; hasRetried = true; doRequestWithLogin(); return; &#125; message = &apos;登录态已过期&apos;; error = new RequestError(data.error, message); callFail(error); return; &#125; else &#123; callSuccess.apply(null, arguments); &#125; &#125;, fail: callFail, complete: noop, &#125;)); &#125;;&#125;; 再看下后端sdk中鉴权中间件代码，检查登录态失效，会返回code为-1，这样client再请求一次，先调用login接口，不用用户去主动登录，这样的话后端sdk的这个鉴权部分是不是便形同虚设了呢。 1234567891011121314151617181920212223242526272829function validation (req) &#123; const &#123; &apos;x-wx-skey&apos;: skey &#125; = req.headers if (!skey) throw new Error(ERRORS.ERR_SKEY_INVALID) debug(&apos;Valid: skey: %s&apos;, skey) return AuthDbService.getUserInfoBySKey(skey) .then(result =&gt; &#123; if (result.length === 0) throw new Error(ERRORS.ERR_SKEY_INVALID) else result = result[0] // 效验登录态是否过期 const &#123; last_visit_time: lastVisitTime, user_info: userInfo &#125; = result const expires = config.wxLoginExpires &amp;&amp; !isNaN(parseInt(config.wxLoginExpires)) ? parseInt(config.wxLoginExpires) * 1000 : 7200 * 1000 if (moment(lastVisitTime, &apos;YYYY-MM-DD HH:mm:ss&apos;).valueOf() + expires &lt; Date.now()) &#123; debug(&apos;Valid: skey expired, login failed.&apos;) return &#123; loginState: LOGIN_STATE.FAILED, userinfo: &#123;&#125; &#125; &#125; else &#123; debug(&apos;Valid: login success.&apos;) return &#123; loginState: LOGIN_STATE.SUCCESS, userinfo: JSON.parse(userInfo) &#125; &#125; &#125;)&#125; 当然，上面sdk源码中的这些问题仅仅是我自己的思考，整体使用起来还是能使用的，如果是使用腾讯云的相关服务，配合这套全栈方案还是不错的。如果开发者用自己的服务器，可选择熟悉的后端语言，参考该流程开发。","categories":[{"name":"前端","slug":"前端","permalink":"https://charmsun.github.io/categories/前端/"}],"tags":[{"name":"小程序","slug":"小程序","permalink":"https://charmsun.github.io/tags/小程序/"}]},{"title":"React + Redux 开发中的选型与优化","slug":"react-redux-selection-optimization","date":"2018-06-28T09:15:51.000Z","updated":"2018-07-29T08:53:02.039Z","comments":true,"path":"2018/06/28/react-redux-selection-optimization/","link":"","permalink":"https://charmsun.github.io/2018/06/28/react-redux-selection-optimization/","excerpt":"","text":"react和redux的思想是开放的，海纳百川，周边生态百花齐放，不同于vue，官方给出的最佳实践几乎是唯一的，而在react + redux开发中根据需求和喜好的不同，可以有不同的组合式选择，社区也有多种优秀实践方案。这里对react + redux开发中选型做一个总结，并探讨一下react + redux开发中的优化问题。 一、React和Redux开发选型目前，React生态圈已十分成熟，Redux也已成为React开发中的主流框架，我们知道，Redux 是一个体小精悍的库，也衍生出了丰富的工具集和可扩展的生态系统。之前看了一篇文章redux 三重境，将redux分为三个境界： 这里就探索一下redux周边生态，Redux第三方库纷呈繁杂，都能针对redux开发中存在一些痛点进行针对性解决，例如：如何优雅的进行异步控制，如何避免重复冗余的actionCreator，如何避免state树细微局部变化而导致一些selector的重复计算等。在开发时，这些第三方库未必都会用到，也有一定学习成本，这里就对一些比较成熟流行的第三方库进行一个归纳总结，提供一个开发选型时的参考。 路由处理 react-router和react-router-reduxreact-router是开发时采用的路由库，它通过管理 URL，实现组件的切换和状态的变化，开发复杂的应用几乎肯定会用到。目两位创始人闹翻了，其中一位另起门户，写了一个功能相同的 Reach router，这里就不详细介绍了。如果把react-router url的变化、参数等信息作为状态，交给redux的store管理，可以使用react-router-redux，一般场景下直接使用react-router即可，因为url的这些状态比较独立，不一定非要交给redux来管理的。 Redux异步控制 redux-thunk 和 redux-saga在使用Redux进行React开发过程中，最常遇到的就是异步操作如何处理的问题。异步方案选型有好几种，有redux-thunk，redux-saga，redux-promise，redux-observable等，这里我们就讨论两种常用的redux-thunk和redux-saga。 我们知道，Redux可以被看作是Flux的一种实现，因此Redux中action对象是参照Flux标准Action（FSA）构造的。什么是FSA（Flux Standard Action）呢，可看如下代码，action需要是一个简单对象，每个key需要是’type’, ‘payload’, ‘error’, ‘meta’中的一个： 1234567891011export function isFSA(action) &#123; return ( isPlainObject(action) &amp;&amp; isString(action.type) &amp;&amp; Object.keys(action).every(isValidKey) );&#125;function isValidKey(key) &#123; return [&apos;type&apos;, &apos;payload&apos;, &apos;error&apos;, &apos;meta&apos;].indexOf(key) &gt; -1;&#125; 官方文档里介绍了一种很朴素的异步控制中间件redux-thunk, 它的代码只有十几行： 1234567891011121314function createThunkMiddleware(extraArgument) &#123; return (&#123; dispatch, getState &#125;) =&gt; next =&gt; action =&gt; &#123; if (typeof action === &apos;function&apos;) &#123; return action(dispatch, getState, extraArgument); &#125; return next(action); &#125;;&#125;const thunk = createThunkMiddleware();thunk.withExtraArgument = createThunkMiddleware;export default thunk; 通过使用该中间件，action可以是函数，把dispatch、getState、extraArgument 作为参数传入这个函数里，实现异步控制。redux-thunk的使用很简单，但也有些问题，action的设计不符合FSA，还有action函数中的异步控制比较复杂，会产生好几个新的action，异步代码中可能到处都是dispatch(action)，不可控。 相比于redux-thunk，redux-saga就要难很多，同时也要优雅很多。官网介绍它是用来处理副作用的redux中间件，老外喜欢搞一些抽象性名词，副作用是什么，我们知道，reducer 都是纯函数，纯函数是指一个函数的返回结果只依赖于它的参数，并且在执行过程中不会对外部产生副作用，即给它传什么，就吐出什么。但是在实际的应用开发中，我们希望做一些异步的（如Ajax请求）且不纯粹的操作（如改变外部的状态），这些在函数式编程范式中被称为“副作用”。 Redux-Saga充分利用了ES6的Generator特性，一切异步调用都可以通过yield交给Redux-Saga去处理，并在异步代码执行完成后返回yield处继续执行，从此和回调形式的异步调用说再见，同时支持try/catch语法，可以更方便的对代码块进行异常捕获。看一个简单的示例代码： 123456789101112131415161718192021222324252627282930313233import &#123; call, put, takeEvery, takeLatest &#125; from &apos;redux-saga/effects&apos;import Api from &apos;...&apos;// worker Saga : 将在 USER_FETCH_REQUESTED action 被 dispatch 时调用function* fetchUser(action) &#123; try &#123; const user = yield call(Api.fetchUser, action.payload.userId); yield put(&#123;type: &quot;USER_FETCH_SUCCEEDED&quot;, user: user&#125;); &#125; catch (e) &#123; yield put(&#123;type: &quot;USER_FETCH_FAILED&quot;, message: e.message&#125;); &#125;&#125;/* 在每个 `USER_FETCH_REQUESTED` action 被 dispatch 时调用 fetchUser 允许并发（译注：即同时处理多个相同的 action）*/function* mySaga() &#123; yield takeEvery(&quot;USER_FETCH_REQUESTED&quot;, fetchUser);&#125;/* 也可以使用 takeLatest 不允许并发，dispatch 一个 `USER_FETCH_REQUESTED` action 时， 如果在这之前已经有一个 `USER_FETCH_REQUESTED` action 在处理中， 那么处理中的 action 会被取消，只会执行当前的*/function* mySaga() &#123; yield takeLatest(&quot;USER_FETCH_REQUESTED&quot;, fetchUser);&#125;export default mySaga; 代码中创建一个 Saga 来监听所有的 USER_FETCH_REQUESTED action，并触发一个 API 调用获取用户数据。可以想像为，一个 saga 就像是应用程序中一个单独的线程，它独自负责处理副作用。相比redux-thunk的优点： 集中处理了所有的异步操作，异步接口部分一目了然 流程拆分更细，用同步的方式写异步代码 action是plain object，符合FSA 方便异步接口的测试 越是用来解决具体问题的技术，使用起来越容易，越高效，学习成本越低；越是用来解决宽泛问题的技术，使用起来越难，学习成本越高。不管是否用得上，你都应该尝试一下。 缩减样板代码redux-actionsRedux中一个约定俗成的做法是通过创建函数生成 action 对象，而不是在你 dispatch 的时候内联生成它们。当应用越来越大之后，写简单的 action creator 很容易让人厌烦，且往往最终生成多余的样板代码。一些工具库也可以帮助生成 action creator ，例如 redux-act 和 redux-actions 。这些库可以有效减少你的样板代码，并紧守例如 Flux Standard Action (FSA) 一类的标准。这里我们主要看看redux-actions，官网示例： 12345678910111213141516171819202122import &#123; createActions, handleActions, combineActions &#125; from &apos;redux-actions&apos;;const defaultState = &#123; counter: 10 &#125;;const &#123; increment, decrement &#125; = createActions(&#123; INCREMENT: (amount = 1) =&gt; (&#123; amount &#125;), DECREMENT: (amount = 1) =&gt; (&#123; amount: -amount &#125;)&#125;);const reducer = handleActions( &#123; [combineActions(increment, decrement)]: ( state, &#123; payload: &#123; amount &#125; &#125; ) =&gt; &#123; return &#123; ...state, counter: state.counter + amount &#125;; &#125; &#125;, defaultState);export default reducer; redux-actions能减少reducer 和 action 中样板代码，能避免写太多action creator函数和reducer中的switch分支写法 提高 selector 的性能reselect在使用redux进行react开发时，我们通常使用react-redux来将两者进行连接，react-redux中的核心方法connect作用就是选取redux store中需要的state与disaptch，交由connect去绑定react组件的props。指定store中的哪些 state 属性被注入到 component 的 props 中，这是通过所谓的 selector 函数完成的，我们注意到，selector是一个纯函数。有一个明显的性能问题是store中state的任何更新都会导致selector的重新计算，这其实是非常不合适的，reselect这个库就能采用缓存帮助省去没必要的重新计算。这也是一个非常简练的库，源代码100多行。看一个使用示例： 12345678910111213141516171819202122232425262728import &#123; createSelector &#125; from &apos;reselect&apos;;import &#123; connect &#125; from &apos;react-redux&apos;;import &#123; toggleTodo &#125; from &apos;../actions&apos;import TodoList from &apos;../components/TodoList&apos;const todoSelector = createSelector( state =&gt; state.todos, todos =&gt; todos.filter(i =&gt; i.completed))const mapStateToProps = (state) =&gt; &#123; todos: todoSelector // 其它props...&#125;const mapDispatchToProps = (dispatch) =&gt; &#123; onTodoClick: (id) =&gt; &#123; dispatch(toggleTodo(id)) &#125;&#125;// 绑定到组件上const VisibleTodoList = connect( mapStateToProps, mapDispatchToProps)(TodoList)export default VisibleTodoList 代码中selector便只会根据 state.todos 变化时重新计算，而不是整个state tree变化都计算。 不可变数据管理redux-immutable在redux的架构中，reducer应该是纯函数，纯函数意味着不能突变（mutate）它的参数，意味着state的更新应该在“不可变（immutable）”的理念下完成。这就是说reducer总是返回一个新的更新后的对象，而不是直接修改原始的state tree。 Immutable.js 这样不可变的库被设计用于克服 JavaScript 中固有的不变性问题，redux中的state是JS对象，不少开发者会使用 Immutable.js等不可变的库将其由JS对象转化为immutable对象，这时就需要结合使用redux-immutable这样的库，类似的库还有redux-immutablejs。用法很简单，需将redux中的combineReducers方法改为使用redux-immutable中的： 1234const StateRecord = Immutable.Record(&#123; foo: &apos;bar&apos;&#125;);const rootReducer = combineReducers(&#123;foo: fooReducer&#125;, StateRecord); 小结其实还有很多redux生态圈的轮子没有提到，例如： redux-promise： redux中间件，用来处理promise redux-rx： 用于引入RxJS的响应式编程思想“轮子永远是造不完的，也是看不完的，这么多轮子的取舍其实终究还是要看开发者的能力以及实际项目的需求，或许你根本不需要这些东西”。 二、react + redux开发中的优化我们在刚接触react开发时，觉得就像官网说的那样，react就意味着组件化、高性能，我们只关注数据整体，至于两次数据之间UI如何变化，完全交给Diff算法去做，所以shouldComponentUpdate很少去写，反正能渲染出想要的结果就行。实际上Virtual DOM算法只是避免了没必要的真实 DOM 操作，所以 React 性能很好。但随着应用复杂度的提升， DOM 树越来越复杂，大量的对比操作也会影响性能。 PureComponentReact15.3 中新加了一个类PureComponent，它和 Component 基本一样，只不过会在 render 之前帮组件自动执行一次shallowEqual（浅比较），来决定是否更新组件，浅比较只会比较第一层。如下： 123456789101112131415161718192021222324252627function shallowEqual(objA, objB) &#123; if (objA === objB) &#123; return true; &#125; if (typeof objA !== &apos;object&apos; || objA === null || typeof objB !== &apos;object&apos; || objB === null) &#123; return false; &#125; var keysA = Object.keys(objA); var keysB = Object.keys(objB); if (keysA.length !== keysB.length) &#123; return false; &#125; // Test for A&apos;s keys different from B. var bHasOwnProperty = Object.prototype.hasOwnProperty.bind(objB); for (var i = 0; i &lt; keysA.length; i++) &#123; if (!bHasOwnProperty(keysA[i]) || objA[keysA[i]] !== objB[keysA[i]]) &#123; return false; &#125; &#125; return true;&#125; 使用 PureComponent 相当于自动加上了 shouldComponentUpdate 函数： 123function shouldComponentUpdate(nextProps, nextState) &#123; return !shallowEqual(this.props, nextProps) || !shallowEqual(this.state, nextState);&#125; 当组件更新时，如果组件的 props 和 state 浅比较都没发生改变， render 方法就不会触发。但如果 props 和 state 值没变，但引用变了，也同样会造成虚拟 DOM 计算的浪费，如果值改了，但引用没改，又会造成不渲染。因而需要很小心地操作数据，这时候就可以搭配Immutable.js进行数据操作。 Immutable.jsImmutable.js同样是 Facebook 出的持久性数据结构的库，用来创建和操作Immutable 数据对象。Immutable数据就是一旦创建，就不能更改的数据。每当对Immutable对象进行修改的时候，就会返回一个新的Immutable对象，以此来保证数据的不可变。关于Immutable.js的介绍和api可以看官网，至于Immutable 是如何使用结构共享来避免深拷贝把所有节点都复制一遍带来的性能损耗的可以看如下图： 通过下面代码可以看出，Immutable对象值不变时，仍然得到原对象，值变化时便是新对象，非常适合与PureComponent配合使用，用来减少组件的重复渲染，用以提高性能。1234567const &#123; Map &#125; = require(&apos;immutable&apos;)const map1 = Map( &#123;a: 1, b: 2, c: 3 &#125;)const map2 = map1.set(&apos;b&apos;, 2)assert.equal(map1, map2) // uses map1.equalsassert.strictEqual(map1, map2) // uses ===const map3 = map1.set(&apos;b&apos;, 50)assert.notEqual(map1, map3) // uses map1.equals Immutable.js结合redux使用的话可以用前面提到的redux-immutable或者redux-immutablejs将整个state tree由js对象改造为Immutable对象。在redux中使用immutable需要注意的是：异步操作和服务器的交互当然采用的是JS原生对象，除此之外的其他位置，避免出现toJS()的转换代码，统一采用immutable 数据，也就是说接收到服务端的数据后，在流转入全局 state 之前，统一转化为 immutable 数据，因为toJS和fromJS这种转换操作会消耗大量的性能。 前面介绍的reselect同样可以优化应用性能，当把整个Redux状态树Immutable化以后，reselect就需要修改映射函数： 12const getTodos = (state) =&gt; &#123;state.get(&apos;todos&apos;)&#125;const getFilter = (state) =&gt; &#123;state.get(&apos;filter&apos;)&#125; 特别需要注意的是在选择器第二步处理函数内，如果涉及Immutable操作，也需要额外修改成Immutable对应方式。 小结ImmutableJS 结合 PureComponent 可以很大程度的减少重复渲染，大量提高性能，并且提供了大量的类似原生 JS 的方法，完全函数式编程，也容易实现时间旅行，但也存在这样一些问题： ImmutableJS 库体积比较大 有学习成本 侵入性强，会渗透整个项目的代码 参考 redux 三重境 Redux的全家桶与最佳实践 Immutable 详解及 React 中实践","categories":[{"name":"前端","slug":"前端","permalink":"https://charmsun.github.io/categories/前端/"}],"tags":[{"name":"react","slug":"react","permalink":"https://charmsun.github.io/tags/react/"},{"name":"redux","slug":"redux","permalink":"https://charmsun.github.io/tags/redux/"},{"name":"Immutable","slug":"Immutable","permalink":"https://charmsun.github.io/tags/Immutable/"}]},{"title":"前端自动化单元测试","slug":"front-end-unit-testing","date":"2018-04-03T08:13:05.000Z","updated":"2018-04-25T10:11:14.402Z","comments":true,"path":"2018/04/03/front-end-unit-testing/","link":"","permalink":"https://charmsun.github.io/2018/04/03/front-end-unit-testing/","excerpt":"","text":"随着Web业务的日益复杂化和多元化，前端开发也有了前端工程化的概念，前端工程化成为目前前端架构中重要的一环，本质上也是软件工程的一种，因此我们需要从软件工程的角度来研究前端工程，而自动化测试则是软件工程中重要的一环。本文就研究一下前端领域中的自动化测试，以及如何实践。 什么是单元测试 单元测试（unit testing），是指对软件中的最小可测试单元进行检查和验证。对于单元测试中单元的含义，一般来说，要根据实际情况去判定其具体含义，如C语言中单元指一个函数，Java里单元指一个类，图形化的软件中可以指一个窗口或一个菜单等。总的来说，单元就是人为规定的最小的被测功能模块。单元测试是在软件开发过程中要进行的最低级别的测试活动，软件的独立单元将在与程序的其他部分相隔离的情况下进行测试。——百度百科 为何要测试以前没有编写和维护测试用例的习惯，在项目的紧张开发周期中也没时间去做这个工作，相信有不少开发人员都不会重视单元测试这项工作。在真正写了一段时间基础组件后，才发现自动化测试有很多好处: 提升代码质量。虽不能说百分百无bug，但至少说明测试用例覆盖到的场景是没有问题的。 能快速反馈，能确定UI组件工作情况是否符合自己预期。 开发者会更加信任自己的代码，也不会惧怕将代码交给别人维护。后人接手一段有测试用例的代码，修改起来也会更加从容。测试用例里非常清楚的阐释了开发者和使用者对于这段代码的期望和要求，也非常有利于代码的传承。 当然由于维护测试用例也是一大笔开销，还是要基于投入产出比来做单元测试。对于像基础组件、基础模型之类的不常变更且复用较多的部分，可以考虑写测试用例来保证质量，但对于迭代较快的业务逻辑及生存时间不长的部分就没必要浪费时间了。 因此github上看到的star较多的牛逼开源前端项目基本上都是有测试代码的，看来业界大牛们都是比较重视单元测试这块的。 相关概念TDDTDD是Test Driven Development 的缩写，也就是测试驱动开发。 通常传统软件工程将测试描述为软件生命周期的一个环节，并且是在编码之后。但敏捷开发大师Kent Beck在2003年出版了 Test Driven Development By Example 一书，从而确立了测试驱动开发这个领域。 TDD需要遵循如下规则： 写一个单元测试去描述程序的一个方面。 运行它应该会失败，因为程序还缺少这个特性。 为这个程序添加一些尽可能简单的代码保证测试通过。 重构这部分代码，直到代码没有重复、代码责任清晰并且结构简单。 持续重复这样做，积累代码。 TDD具有很强的目的性，在直接结果的指导下开发生产代码，然后不断围绕这个目标去改进代码，其优势是高效和去冗余的。所以其特点应该是由需求得出测试，由测试代码得出生产代码。打个比方就像是自行车的两个轮子，虽然都是在向同一个方向转动，但是后轮是施力的，带动车子向前，而前轮是受力的，被向前的车子带动而转。 BDD所谓的BDD行为驱动开发，即Behaviour Driven Development，是一种新的敏捷开发方法。它更趋向于需求，需要共同利益者的参与，强调用户故事（User Story）和行为。2009年，在伦敦发表的“敏捷规格，BDD和极限测试交流”中，Dan North对BDD给出了如下定义： BDD是第二代的、由外及内的、基于拉(pull)的、多方利益相关者的(stakeholder)、多种可扩展的、高自动化的敏捷方法。它描述了一个交互循环，可以具有带有良好定义的输出（即工作中交付的结果）：已测试过的软件。 它对TDD的理念进行了扩展，在TDD中侧重点偏向开发，通过测试用例来规范约束开发者编写出质量更高、bug更少的代码。而BDD更加侧重设计，其要求在设计测试用例的时候对系统进行定义，倡导使用通用的语言将系统的行为描述出来，将系统设计和测试用例结合起来，从而以此为驱动进行开发工作。 大致过程： 从业务的角度定义具体的，以及可衡量的目标 找到一种可以达到设定目标的、对业务最重要的那些功能的方法 然后像故事一样描述出一个个具体可执行的行为。其描述方法基于一些通用词汇，这些词汇具有准确无误的表达能力和一致的含义。例如，expect, should, assert 寻找合适语言及方法，对行为进行实现 测试人员检验产品运行结果是否符合预期行为。最大程度的交付出符合用户期望的产品，避免表达不一致带来的问题 覆盖率如何衡量测试脚本的质量呢？其中一个参考指标就是代码覆盖率（coverage）。 什么是代码覆盖率？简而言之就是测试中运行到的代码占所有代码的比率。其中又可以分为行数覆盖率，分支覆盖率等。具体的含义不再细说，有兴趣的可以自行查阅资料。 虽然并不是说代码覆盖率越高，测试的脚本写得越好，但是代码覆盖率对撰写测试脚本还是有一定的指导意义的。 前端单测工具栈测试框架主要提供了清晰简明的语法来描述测试用例，以及对测试用例分组，测试框架会抓取到代码抛出的AssertionError，并增加一大堆附加信息，比如那个用例挂了，为什么挂等等。目前比较流行的测试框架有： Jasmine： 自带断言（assert）,mock功能 Mocha： 框架不带断言和mock功能，需要结合其他工具，由tj大神开发 Jest： 由Facebook出品的测试框架，在Jasmine测试框架上演变开发而来 断言库断言库提供了很多语义化的方法来对值做各种各样的判断。 chai: 目前比较流行的断言库，支持 TDD（assert），BDD（expect、should）两种风格 should.js：也是tj大神所写 mock库 sinon.js：使用Sinon，我们可以把任何JavaScript函数替换成一个测试替身。通过配置，测试替身可以完成各种各样的任务来让测试复杂代码变得简单。支持 spies, stub, fake XMLHttpRequest, Fake server, Fake time，很强大 测试集成管理工具 karma：Google Angular 团队写的，功能很强大，有很多插件。可以连接真实的浏览器跑测试。能够用一些测试覆盖率统计的工具统计一下覆盖率；或是能够加入持续集成，提交代码后自动跑测试用例。 测试脚本的写法通常，测试脚本与所要测试的源码脚本同名，但是后缀名为.test.js（表示测试）或者.spec.js（表示规格）。 123456789// add.test.jsvar add = require(&apos;./add.js&apos;);var expect = require(&apos;chai&apos;).expect;describe(&apos;加法函数的测试&apos;, function() &#123; it(&apos;1 加 1 应该等于 2&apos;, function() &#123; expect(add(1, 1)).to.be.equal(2); &#125;);&#125;); 上面这段代码，就是测试脚本，它可以独立执行。测试脚本里面应该包括一个或多个describe块，每个describe块应该包括一个或多个it块。 describe块称为”测试套件”（test suite），表示一组相关的测试。它是一个函数，第一个参数是测试套件的名称（”加法函数的测试”），第二个参数是一个实际执行的函数。 describe干的事情就是给测试用例分组。为了尽可能多的覆盖各种情况，测试用例往往会有很多。这时候通过分组就可以比较方便的管理（这里提一句，describe是可以嵌套的，也就是说外层分组了之后，内部还可以分子组）。另外还有一个非常重要的特性，就是每个分组都可以进行预处理（before、beforeEach）和后处理（after, afterEach）。 it块称为”测试用例”（test case），表示一个单独的测试，是测试的最小单位。它也是一个函数，第一个参数是测试用例的名称（”1 加 1 应该等于 2”），第二个参数是一个实际执行的函数。 大型项目有很多测试用例。有时，我们希望只运行其中的几个，这时可以用only方法。describe块和it块都允许调用only方法，表示只运行某个测试套件或测试用例。此外，还有skip方法，表示跳过指定的测试套件或测试用例。1234567describe.only(&apos;something&apos;, function() &#123; // 只会跑包在里面的测试&#125;)it.only(&apos;do do&apos;, () =&gt; &#123; // 只会跑这一个测试&#125;) react 单测示例一react 单元测试框架 demo1 该框架采用 karma + mocha + chai + sinon 的组合， 是一种采用工具较多，同时自由度较高的解决方案。虽然工具库使用的较多，但有助于理解各个工具库的作用和使用，也有助于加深对前端单元测试的理解。 其中React的测试库使用 enzyme，React测试必须使用官方的测试工具库，但是它用起来不够方便，所以有人做了封装，推出了一些第三方库，其中Airbnb公司的Enzyme最容易上手。 关于该库的 api 使用可参考： 官方文档 阮老师React测试入门 react 单测示例二react 单元测试框架 demo2 该框架只采用了Jest，是比较简洁的方案，同样也使用了 enzyme。 Jest 是Facebook开发的一个测试框架，它集成了测试执行器、断言库、spy、mock、snapshot和测试覆盖率报告等功能。React项目本身也是使用Jest进行单测的，因此它们俩的契合度相当高。之前仅在其内部使用，后开源，并且是在Jasmine测试框架上演变开发而来，使用了熟知的expect(value).toBe(other)这种断言格式。 PS: 目前 enzyme 使用时需要加入设置如下：1234import Enzyme from &apos;enzyme&apos;;import Adapter from &apos;enzyme-adapter-react-16&apos;;Enzyme.configure(&#123; adapter: new Adapter() &#125;); 上面两个框架方案中都有加入该配置的方法，详见示例。 参考 聊一聊前端自动化测试 前端自动化单元测试初探 Javascript的Unit Test 单元测试all in one Sinon指南: 使用Mocks, Spies 和 Stubs编写JavaScript测试 测试框架 Mocha 实例教程","categories":[{"name":"前端","slug":"前端","permalink":"https://charmsun.github.io/categories/前端/"}],"tags":[{"name":"前端单元测试","slug":"前端单元测试","permalink":"https://charmsun.github.io/tags/前端单元测试/"}]},{"title":"前端持续集成","slug":"front-end-ci","date":"2018-04-03T07:18:51.000Z","updated":"2018-05-16T08:29:11.127Z","comments":true,"path":"2018/04/03/front-end-ci/","link":"","permalink":"https://charmsun.github.io/2018/04/03/front-end-ci/","excerpt":"","text":"编写代码只是软件开发的一小部分，更多的时间往往花在构建（build）和测试（test）。之前介绍了前端的单元测试，这篇再介绍和实践一下前端的持续集成（Continuous integration），简写为CI。 什么是持续集成 持续集成是一种软件开发实践，即团队开发成员经常集成他们的工作，通过每个成员每天至少集成一次，也就意味着每天可能会发生多次集成。每次集成都通过自动化的构建（包括编译，发布，自动化测试）来验证，从而尽早地发现集成错误。 持续集成的益处： 减少风险 减少重复过程 任何时间、任何地点生成可部署的软件 增强项目的可见性 建立团队对开发产品的信心 集成工具GitHub 上比较主流的持续集成工具有 Travis CI 和 Circle CI；分别占市场占有率前两位，两者的功能和使用都比较相似，其中Travis CI用的最多，就选择Travis CI进行介绍。 Travis CI 只支持 Github，不支持其他代码托管服务。这意味着，你必须满足以下条件，才能使用 Travis CI。 拥有 GitHub 帐号 该帐号下面有一个项目 该项目里面有可运行的代码 该项目还包含构建或测试脚本 Travis有两个网址，org的那个是非盈利的，为GitHub上public的repository提供免费服务；com的那个是盈利的，可以对private的提供付费服务。com前100次build是免费的，此后按月收费。 使用很简单，用GitHub 账号登录后,Travis 会列出 Github 上面你的所有仓库，以及你所属于的组织。此时，选择你需要 Travis 帮你构建的仓库，打开仓库旁边的开关。一旦激活了一个仓库，Travis 会监听这个仓库的所有变化。 流程很简明，但绝大多数复杂度都集中在这个.travis.yml文件。这是一个YAML文件，主要用来做CI的配置。Travis会按照这个文件配置的方式来运行。以下是项目中使用travis的简单例子：123456789101112131415161718language: node_jsnode_js: - &quot;8&quot;install: - npm install -g codecov - npm installbefore_script: - &quot;export DISPLAY=:99.0&quot; - &quot;sh -e /etc/init.d/xvfb start&quot; - sleep 3 # give xvfb some time to startscript: - cross-env TRAVIS=true karma start --single-run - codecov Travis 的运行流程很简单，任何项目都会经过两个阶段。 12- install 阶段：安装依赖- script 阶段：运行脚本 Node 项目的install和script阶段都有默认脚本，可以省略。 12- install 默认值：npm install- script 默认值：npm test 完整的生命周期，从开始到结束是下面的流程: 1234567891. before_install2. install3. before_script4. script5. aftersuccess or afterfailure6. [OPTIONAL] before_deploy7. [OPTIONAL] deploy8. [OPTIONAL] after_deploy9. after_script 更多用法可查阅官方文档，官方文档比较详细。 在上面的例子中，每次代码 push 以后，Travis 会自动开始构建，并运行单测，最后得到构建状态如下 点击状态图标，可以在弹出界面中得到该项目的状态图标链接。放到repository的README.md中，就可以在GitHub页面得到编译状态的展示了。 另外，如果多次提交同时push，默认只在最新提交执行一次build；在git commit中如果包含[skip ci]或[ci skip]，该提交就不会触发一次build。 集成测试覆盖率工具代码覆盖率报告可以为编写测试程序提供参考，通过一些工具，还可以及时的把你的代码的测试情况及时的反馈给用户，让用户感知你的测试是否完备。 GitHub 上比较主流的代码覆盖率集成工具有 codecov 和 coveralls；两者也差不多，在之前travis的例子中选择使用了codecov。 同样使用GitHub 账号登录，第一次使用时，默认是没有 repository 的，需要通过点击 + Add my first repository 来添加需要 codecov 管理的 repository。无论 codecov 还是 coveralls，它自身都不会去运行测试用例来获得项目代码的覆盖率，而是通过收集覆盖率报告及其他关键信息来静态分析。codecov 可以接收 lcov, gcov 以及正确的 json 数据格式作为输入信息。 该例子就是在travis CI 平台中跑完测试用例后，将测试报告中的 lcov 和 json 文件提交至codecov 平台进行覆盖率分析，同样分析的覆盖率结果的徽章图标也能从平台中获取，可以加入到GitHub repository 的README.md中。 123456789101112131415161718language: node_jsnode_js: - &quot;8&quot;install: - npm install -g codecov - npm installbefore_script: - &quot;export DISPLAY=:99.0&quot; - &quot;sh -e /etc/init.d/xvfb start&quot; - sleep 3 # give xvfb some time to startscript: - cross-env TRAVIS=true karma start --single-run - codecov 跨浏览器集成测试浏览器端使用的库，在各个浏览器端的兼容性也是非常重要的。跨浏览器测试同样有 2 个选择，SAUCELABS 和 BrowserStack，可以进行浏览器兼容性测试。这里没有详细实践过，怎么操作可以去参考文章看。 至此，CI 除了能跑跑测试、显示覆盖率之外，还能自动部署发布，怎样部署也在 yaml文件中进行脚本编写，这里就不举例子了。以上说的都是源代码放在 Github 上的开源代码，但相信我们接触得更多的应该是公司的私有代码，私有代码的项目也有 CI 解决方案，就是Jenkins，其前身是 Hudson。 Jenkins这里先搭建了一个 Jenkins， 搭建过程可自行搜索，说下能解决什么问题。比如有这样的场景： 前后端分离后希望将前端某版本代码部署至服务器，供联调，查看构建过程等用途； 每次代码提交后都能自动构建自动部署，无须任何操作； 项目紧张时，领导希望能看到每天程序员提交代码后，项目的进展情况等。 这些场景下， Jenkins 都能帮助解决，凡是需要自动构建自动部署的，它都能帮上忙。这里举例大致说下怎么使用，就以将某个业务系统前端代码自动构建部署至nginx服务器为例。 第一步：新建项目，左边菜单第一项。 第二步：进行相关配置，填写源码配置，可选择仓库源，git和svn都支持。 第三步：进行构建触发配置，可设置触发构建的情形，例如代码提交时构建，按日程表构建等。 第四步：填写构建部署脚本。 第五步：保存并进行构建，查看构建情况。 详细配置可以上Jenkins中查看，构建过程包括自动更新svn代码，安装或更新npm包，进行webpack打包，将打包后的代码部署至静态资源服务器。至此完成前端代码的构建和部署，每次提交代码，构建过程全自动运行，当状态指示灯变蓝时就表示构建成功，我们可以访问服务器前端资源了。 这只是一个比较简单的例子，还有不少其他功能，Jenkin有很多插件，总之很强大。当然不同的项目可能会有不同的构建过程，可以自己按需设定构建过程。 结语结尾来个段子： 以前我坚持认为代码应该是 clean 的，架构应该是简洁的，流程应该是自动化的，文档应该是齐全的，技术是应该分享的。 效率较高不加班，结果 kpi 被差评，领导认为我无所事事。 最近半年我换了风格，架构重度设计，类名变量模糊不清，没有注释，现在大家都离不开我了，kpi 也上去了。 段子笑一笑而已，最后还是希望能运用到前面的持续集成工具来实现流程自动化，可以将一些重复性的工作尽量交给自动化工具来帮我们解决。 参考 前端开源项目持续集成三剑客 持续集成服务 Travis CI 教程 前端持续集成解决方案 从GitHub到Travis","categories":[{"name":"前端","slug":"前端","permalink":"https://charmsun.github.io/categories/前端/"}],"tags":[{"name":"前端工程化","slug":"前端工程化","permalink":"https://charmsun.github.io/tags/前端工程化/"},{"name":"持续集成","slug":"持续集成","permalink":"https://charmsun.github.io/tags/持续集成/"}]},{"title":"gulp使用小结","slug":"gulp-summary","date":"2016-07-08T07:18:51.000Z","updated":"2018-05-16T09:26:33.777Z","comments":true,"path":"2016/07/08/gulp-summary/","link":"","permalink":"https://charmsun.github.io/2016/07/08/gulp-summary/","excerpt":"","text":"前段时间做了一个项目，使用了gulp来实现广告样式模板文件的自动化构建和热更新开发，对gulp的使用体验感受是，插件丰富，构建流程易于控制，使用简单，构建效率也不错，相比于同样插件体系丰富的grunt和webpack构建机制而言，由于gulp其实是整合众多的gulp插件来编写Node.js可执行任务脚本，因此构建过程中构建流程对开发人员更清晰，也更易于编码进行控制，自定义化更强。当然，gulp与webpack并不冲突，webpack的模块化是gulp没有的，两者可结合使用。 本文不对gulp做过多介绍，介绍可以看官网，就只总结一些常用gulp插件的用途和用法，希望起到参考作用。 插件介绍Gulp现在大约有2000多左右的插件，可以到http://gulpjs.com/plugins/或者http://npmsearch.com/?q=keywords:gulpplugin查找。这里列出一些出场率较高或者我了解的插件，并可能在将来的开发中更新。 gulp-jshintgulp的jshint插件，用来检测js代码中语法和代码风格问题。其中jshint有两个参数： lookup: 默认是true, 为false时不查找.jshintrc 文件 linter: 可以指定其他的linter， 例如 “jsxhint” jshint.reporter 可以指定错误报告的形式 gulp-jshint用法： 1234567var jshint = require(&apos;gulp-jshint&apos;);// 检查脚本gulp.task(&apos;lint&apos;, function() &#123; return gulp.src(config.scripts) .pipe(jshint()) .pipe(jshint.reporter(&apos;default&apos;)); // 输出检查结果&#125;); 123456gulp.task(&apos;lint&apos;, function() &#123; return gulp.src([&apos;src/*.js&apos;, &apos;src/ui/*.js&apos;]) .pipe(jshint()) .pipe(jshint.reporter(&apos;jshint-stylish&apos;)) //输出结果会带颜色 .pipe(jshint.reporter(&apos;fail&apos;)); //检查有错则任务失败&#125;); gulp-concat用来把多个文件合并为一个文件,我们可以用它来合并js或css文件等，这样就能减少页面的http请求数了 gulp-concat用法： 123456var concat = require(&apos;gulp-concat&apos;);gulp.task(&apos;scripts&apos;, function() &#123; gulp.src(&apos;./lib/*.js&apos;) .pipe(concat(&apos;all.js&apos;)) .pipe(gulp.dest(&apos;./dist/&apos;))&#125;); gulp-uglify用来压缩js文件，使用的是uglify引擎 gulp-uglify 用法： 123456var uglify = require(&apos;gulp-uglify&apos;);gulp.task(&apos;compress&apos;, function() &#123; gulp.src(&apos;lib/*.js&apos;) .pipe(uglify()) .pipe(gulp.dest(&apos;dist&apos;))&#125;); gulp-rename改变管道中的文件名。 gulp-rename 用法： 1234567891011var concat = require(&apos;gulp-concat&apos;);// rename via hashgulp.src(&quot;./src/main/text/hello.txt&quot;, &#123; base: process.cwd() &#125;) .pipe(rename(&#123; dirname: &quot;main/text/ciao&quot;, basename: &quot;aloha&quot;, prefix: &quot;bonjour-&quot;, suffix: &quot;-hola&quot;, extname: &quot;.md&quot; &#125;)) .pipe(gulp.dest(&quot;./dist&quot;)); // ./dist/main/text/ciao/bonjour-aloha-hola.md gulp-clean-css用来压缩css文件，减小文件大小，之前gulp-minify-css已经被废弃，现在都使用gulp-clean-css，用法一致，最终是调用clean-css，使用参数请看这里。 gulp-if有条件的执行任务 gulp-ftp将文件上传至ftp服务器 gulp-clean-css、gulp-if、gulp-ftp 用法： 12345678910111213141516171819var cleanCSS = require(&apos;gulp-clean-css&apos;);var concat = require(&apos;gulp-concat&apos;);var ftp = require(&apos;gulp-ftp&apos;);var gulpif = require(&apos;gulp-if&apos;);var CDN_CONFIG = &#123; host: &apos;ftp.cdn.qcloud.com&apos;, user: &apos;1251479438&apos;, pass: &apos;*******&apos;, remotePath: &apos;/0/brandad&apos;&#125;;// 合并、压缩cssgulp.task(&apos;css&apos;, function() &#123; return gulp.src(config.styles) .pipe(concat(config.id + &apos;.css&apos;)) .pipe(cleanCSS()) .pipe(gulp.dest(&apos;./&apos; + config.id + &apos;/dist&apos;)) .pipe( gulpif(config.cssCDN, ftp(CDN_CONFIG)) ); //根据配置判断是否使用ftp上传至cdn服务器&#125;); gulp-less 和 gulp-sass分别用来编译less和sass 用法： 12345678var gulp = require(&apos;gulp&apos;), less = require(&quot;gulp-less&quot;); gulp.task(&apos;compile-less&apos;, function () &#123; gulp.src(&apos;less/*.less&apos;) .pipe(less()) .pipe(gulp.dest(&apos;dist/css&apos;));&#125;); 12345678var gulp = require(&apos;gulp&apos;), sass = require(&quot;gulp-sass&quot;); gulp.task(&apos;compile-sass&apos;, function () &#123; gulp.src(&apos;sass/*.sass&apos;) .pipe(sass()) .pipe(gulp.dest(&apos;dist/css&apos;));&#125;); gulp-css-base64用来将图片转成base64 用法： 1234567var cssBase64 = require(&apos;gulp-css-base64&apos;);gulp.task(&apos;img2base64&apos;, function() &#123; return gulp.src(&apos;node_modules/jstree/dist/themes/default/style.css&apos;) .pipe(cssBase64()) .pipe(rename(&apos;style.base64.css&apos;)) .pipe(gulp.dest(&apos;node_modules/jstree/dist/themes/default&apos;));&#125;); gulp-sequence 和 run-sequencegulp 的 task 都是并行(异步)执行，如果遇见需要串行的场景，那么这个插件就可以解决问题。不过好像gulp4.0发布后，不需要RS也可以搞定串行任务了。 用法： 1234var gulp = require(&apos;gulp&apos;)var gulpSequence = require(&apos;gulp-sequence&apos;) gulp.task(&apos;test&apos;, gulpSequence([&apos;a&apos;, &apos;b&apos;], &apos;c&apos;, [&apos;d&apos;, &apos;e&apos;], &apos;f&apos;)) gulp-inject-string用来在管道中对文件拼接字符串，更多用法可参见这里 用法： 1234567891011121314151617var gulp = require(&apos;gulp&apos;), rename = require(&apos;gulp-rename&apos;), inject = require(&apos;gulp-inject-string&apos;); gulp.task(&apos;inject:append&apos;, function()&#123; gulp.src(&apos;src/example.html&apos;) .pipe(inject.append(&apos;\\n&lt;!-- Created: &apos; + Date() + &apos; --&gt;&apos;)) .pipe(rename(&apos;append.html&apos;)) .pipe(gulp.dest(&apos;build&apos;));&#125;); gulp.task(&apos;inject:prepend&apos;, function()&#123; gulp.src(&apos;src/example.html&apos;) .pipe(inject.prepend(&apos;&lt;!-- Created: &apos; + Date() + &apos; --&gt;\\n&apos;)) .pipe(rename(&apos;prepend.html&apos;)) .pipe(gulp.dest(&apos;build&apos;));&#125;); 还有一些插件简略说明，可自行搜索： gulp-rev ：把静态文件名改成hash的形式。 gulp-sourcemaps ： 处理 JavaScript 时生成 SourceMap。 gulp-markdown ：写手的福音，可以把 Markdown 转成 HTML。 gulp-autoprefixer：给 CSS增加前缀。解决某些CSS属性不是标准属性，有各种浏览器前缀的情况。 启动服务和实时构建在实际项目中，可以利用browser-sync来启动服务，并进行事件监听实时构建，热更新。一旦修改，即刻构建并渲染数据刷新页面，在开发过程中非常方便。 1234567891011gulp.task(&apos;default&apos;, [&apos;render-ad&apos;], function() &#123; browserSync.init(&#123; server: &#123; baseDir: &apos;./&apos; &#125; &#125;); gulp.watch(config.styles, [&apos;render-ad&apos;]); gulp.watch(config.scripts, [&apos;render-ad&apos;]); gulp.watch(config.tpl, [&apos;render-ad&apos;]); gulp.watch(config.data, [&apos;render-ad&apos;]);&#125;);","categories":[{"name":"前端","slug":"前端","permalink":"https://charmsun.github.io/categories/前端/"}],"tags":[{"name":"gulp","slug":"gulp","permalink":"https://charmsun.github.io/tags/gulp/"}]},{"title":"用PL/Proxy为PostgreSQL配置数据库集群","slug":"plproxy","date":"2015-07-08T07:00:18.000Z","updated":"2018-05-16T09:22:00.923Z","comments":false,"path":"2015/07/08/plproxy/","link":"","permalink":"https://charmsun.github.io/2015/07/08/plproxy/","excerpt":"","text":"最近学习使用 PL/Proxy 和 PostgreSQL 来做数据库集群，以免将来忘记，特将学习笔记记录如下： 实验环境为：Fedora 16（三台虚拟机），Postgres 9.1.7，plproxy-2.5 原理：以三台机器的集群为例子，看看PostgreSQL集群的架构是什么。 proxy节点：proxy节点实际上也是一个PostgreSQL数据库 节点，但是所有数据均不存放到proxy节点上，主要做三件事情： 接受用户的sql查询； 分析用户的sql查询并转换成集群上执行的SQL语句； 合并集群执行sql的结果，然后返回给用户。 说白了，就是把用户的sql语句交给database0，database1去执行，然后合并执行结果返回给用户。 database0 节点和 database1节点：就是普通的数据库节点，接收proxy节点的sql查询请求并返回结果给proxy节点，是真正存放数据的节点。 步骤： 在Fedora 16上安装postgresql数据库：yum install postgresql-server.x86_64，安装后会创建一个postgres的用户，用户目录在/var/lib/pgsql，需要切换到postgres用户执行initdb命令初始化数据库，可以查看用户目录startpg.sh的脚本，可以修改数据库data数据文件位置，执行该脚本便可以完成数据库服务的启动。 从官网下载plproxy安装包，解压后查看其中的README文本，可以大概知道怎么安装，不过先别急，你此时按照README所述执行make 和make install命令可能出错，因为经过我的摸索，在fedora下安装这个需要依赖一些其他软件包。当然，按照参考资料中直接在ubantu下从软件仓库安装postgresql-X.X-plproxy可能例外，不需这么麻烦，但是在fedora yum的仓库中没有，所以需要从官网下载独立安装包。 安装plproxy-2.5之前，输入pg_config命令查看一下postgresql的一些配置，我的如下： 1234567891011121314BINDIR = /usr/binDOCDIR = /usr/share/doc/pgsqlHTMLDIR = /usr/share/doc/pgsqlINCLUDEDIR = /usr/includePKGINCLUDEDIR = /usr/include/pgsqlINCLUDEDIR-SERVER = /usr/include/pgsql/serverLIBDIR = /usr/lib64PKGLIBDIR = /usr/lib64/pgsqlLOCALEDIR = /usr/share/localeMANDIR = /usr/share/manSHAREDIR = /usr/share/pgsqlSYSCONFDIR = /etcPGXS = /usr/lib64/pgsql/pgxs/src/makefiles/pgxs.mk...... 用echo $PATH查看一下，path中是否包含BINDIR，没有的话需要加上哦。然后需要安装postgresql-devel.x86_64 ，你可以阅读plproxy解压目录下的makefile文件，能大致知道需要一些什么东西。postgresql-devel.x86_64 便提供了配置中 PGXS 所指向的文件，然后还需要安装 flex 和BISON 这两个，然后到plproxy-2.5目录下便可执行 make 和 make install命令了，在/usr/lib64/pgsql/目录中有了plproxy.so这个文件就表示你安装成功了。 安装就绪了就开始配置吧，在$SHAREDIR(/usr/share/pgsql)目录下有个安装plproxy安装成功后生成的extension文件夹，该文件夹中有个plproxy–2.5.0.sql脚本，你可以先阅读看看，知道它要干什么，然后在我们的proxy数据节点中执行该脚本。1#sudo -u postgres /usr/bin/psql –f /usr/share/pgsql/contrib/extension/plproxy.sql plproxy（数据库名） （对了，还需要修改数据库允许连接的配置，在data文件夹下的pg_hba.conf和postgresql.conf两个文件，修改监听地址和IP认证，具体做法不赘述，自己可以搜。） 在三台机器上分别创建三个数据库，分别是proxy,database0,database1，我用的创建工具是pgadmin3图形界面工具，也推荐使用，用命令行也可以，我嫌麻烦。 在pgadmin3中选择proxy数据库，打开query tool，输入如下语句并执行create schema plproxy 在pgadmin3中选择proxy数据库的plproxy模式（刚创建的），打开query tool ，输入如下建立3个函数的语句并执行。 (1)建立plproxy.get_cluster_config函数： 123456789101112131415CREATE OR REPLACE FUNCTION plproxy.get_cluster_config(IN cluster_name text, OUT key text, OUT val text) RETURNS SETOF record AS$BODY$BEGINkey := &apos;statement_timeout&apos;;--就是给 key 变量赋值,赋的值为’statement_timeout’val := 60;--就是给 val 变量赋值,赋的值为 60RETURN NEXT;RETURN;END;$BODY$ LANGUAGE plpgsql VOLATILE COST 100 ROWS 1000;ALTER FUNCTION plproxy.get_cluster_config(text) OWNER TO postgres; (2)建立plproxy.get_cluster_partitions函数： 1234567891011121314151617CREATE OR REPLACE FUNCTION plproxy.get_cluster_partitions(cluster_name text) RETURNS SETOF text AS$BODY$BEGINIF cluster_name = &apos;testcluster&apos; THEN --cluster_name 是群集的名字RETURN NEXT &apos;dbname=database0 host=127.0.0.1&apos;; --数据库节点的数据库名和 IP 地址RETURN NEXT &apos;dbname=database1 host=127.0.0.1&apos;; --数据库节点的数据库名和 IP 地址RETURN;END IF;RAISE EXCEPTION &apos;Unknown cluster&apos;; --如果群集名不存在,抛出异常,这个是在数据库内部处理的,最终会写入日志中。‘Unknown cluster’是报错信息END;$BODY$ LANGUAGE plpgsql VOLATILE COST 100 ROWS 1000;ALTER FUNCTION plproxy.get_cluster_partitions(text) OWNER TO postgres; (3)建立 plproxy.get_cluster_version函数： 1234567891011121314CREATE OR REPLACE FUNCTION plproxy.get_cluster_version(cluster_name text) RETURNS integer AS$BODY$BEGINIF cluster_name = &apos;testcluster&apos; THENRETURN 1;END IF;RAISE EXCEPTION &apos;Unknown cluster&apos;;END;$BODY$ LANGUAGE plpgsql VOLATILE COST 100;ALTER FUNCTION plproxy.get_cluster_version(text) OWNER TO postgres; 在pgadmin3中选择proxy数据库的public模式（默认就有的），打开query tool ，输入如下建立3个函数的语句并执行 (1)建立public.ddlexec(sql_request text)函数： 1234567891011CREATE OR REPLACE FUNCTION ddlexec(query text) RETURNS SETOF integer AS$BODY$CLUSTER &apos;testcluster&apos;;RUN ON ALL;$BODY$ LANGUAGE plproxy VOLATILE COST 100 ROWS 1000;ALTER FUNCTION ddlexec(text) OWNER TO postgres; (2)建立public.dmlexec(sql_request text)函数： 1234567891011CREATE OR REPLACE FUNCTION dmlexec(query text) RETURNS SETOF integer AS$BODY$CLUSTER &apos;testcluster&apos;;RUN ON ANY;$BODY$ LANGUAGE plproxy VOLATILE COST 100 ROWS 1000;ALTER FUNCTION dmlexec(text) OWNER TO postgres; (3)建立public.dqlexec(sql_request text)函数： 1234567891011CREATE OR REPLACE FUNCTION dqlexec(query text) RETURNS SETOF record AS$BODY$CLUSTER &apos;testcluster&apos;;RUN ON ALL;$BODY$ LANGUAGE plproxy VOLATILE COST 100 ROWS 1000;ALTER FUNCTION dqlexec(text) OWNER TO postgres; 在pgadmin3中选择database0 和database1数据库的public模式（默认就有的），打开query tool ，输入如下建立3个函数的语句并执行 (1)建立public.ddlexec(sql_request text)函数： 123456789101112CREATE OR REPLACE FUNCTION ddlexec(query text)RETURNS integer AS$BODY$declareret integer;beginexecute query;return 1;end;$BODY$LANGUAGE &apos;plpgsql&apos; VOLATILECOST 100; (2)建立public.dmlexec(sql_request text)函数： 123456789101112CREATE OR REPLACE FUNCTION dmlexec(query text)RETURNS integer AS$BODY$declareret integer;beginexecute query;return 1;end;$BODY$LANGUAGE &apos;plpgsql&apos; VOLATILECOST 100; (3)建立public.dqlexec(sql_request text)函数： 123456789101112131415CREATE OR REPLACE FUNCTION dqlexec(query text)RETURNS SETOF record AS$BODY$declareret record;beginfor ret in execute query loopreturn next ret;end loop;return;end;$BODY$LANGUAGE &apos;plpgsql&apos; VOLATILECOST 100ROWS 1000; 到此为止，集群就创建成功了，现在就可以通过proxy这个节点对 database0 和database1 进行建表、插入数据、查询数据的操作了 (1)创建表usertable： 在pgadmin3中选择proxy数据库，打开query tool ，输入如下建立usertable的语句并执行1select ddlexec(&apos;create table usertable(id integer)&apos;); 上述语句执行完，在database0和database1数据库的public模式中就应该有usertable这个表了，如果没有这个表，那就是前面某一步出错了啦。 (2)在usertable表中插入数据： 在pgadmin3中选择proxy数据库，打开query tool ，输入如下建立usertable的语句并执行1234567891011select dmlexec(&apos;insert into usertable values(0)&apos;);select dmlexec(&apos;insert into usertable values(1)&apos;);select dmlexec(&apos;insert into usertable values(2)&apos;);select dmlexec(&apos;insert into usertable values(3)&apos;);select dmlexec(&apos;insert into usertable values(4)&apos;);select dmlexec(&apos;insert into usertable values(5)&apos;);select dmlexec(&apos;insert into usertable values(6)&apos;);select dmlexec(&apos;insert into usertable values(7)&apos;);select dmlexec(&apos;insert into usertable values(8)&apos;);select dmlexec(&apos;insert into usertable values(9)&apos;);select dmlexec(&apos;insert into usertable values(10)&apos;); 上述语句执行完之后能看到database0和database1这两个数据库的usertable表中已经有10行数据了啦。 (3)搜索usertable表中数据： 在pgadmin3中选择proxy数据库，打开query tool ，输入如下建立usertable的语句并执行1select * from dqlexec(&apos;select * from usertable &apos;) as (id integer); 获得结果。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://charmsun.github.io/categories/数据库/"}],"tags":[{"name":"PL/Proxy","slug":"PL-Proxy","permalink":"https://charmsun.github.io/tags/PL-Proxy/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://charmsun.github.io/tags/PostgreSQL/"}]}]}